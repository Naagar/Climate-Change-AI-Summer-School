# -*- coding: utf-8 -*-
"""Tracking_Emissions_from_ML_Models_(Revised).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/climatechange-ai-tutorials/tracking-ml-emissions/blob/main/Tracking_Emissions_from_ML_Models_(Revised).ipynb

# How to Track Emissions when Training Machine Learning Models
**Content Creator**: Mel Hanna (mdriscollhanna@gmail.com)

Welcome to CCAI's tutorial on tracking and mitigating the emissions from ML model training!

In order to make edits to this notebook, you should press **File > "Save a Copy in Drive"**. This will ensure that any edits will be on your local copy, and they will not affect the notebook shared with everyone else.

# Table of Contents


*   [Overview](#overview)
*   [Target Audience](#target-audience)
*   [Background & Prerequisites](#background-and-prereqs)
*   [Software Requirements](#software-requirements)
*   [Climate Impact](#climate-impact)
*   [Methodology](#methodology)
*   [Results & Discussion](#codecarbon-experiments)
*   [References](#references)

<a name="overview"></a>
# Overview
This tutorial explains the methodology behind calculating computing-related GHG emissions from training machine learning models and demonstrates some strategies to reduce a model's carbon footprint.

Specifically, you will learn how to:
- measure a ML model's emissions with CodeCarbon
- potentially reduce a ML model's carbon footprint during training

<a name="target-audience"></a>
## **Target Audience**
This tutorial is intended for experienced and aspiring data scientists looking for concrete examples of how to track carbon emissions while executing code and how to train machine learning models in a more efficient way. The reader should understand the basic mechanics of training an ML model, such as splitting data between train and test sets and tuning hyperparameters.

<a name="software-requirements"></a>
## **Software Requirements**

This notebook requires Python >= 3.7. The following libraries are required:
*   tqdm
*   pandas
*   numpy
*   matplotlib
*   pytorch

### **Enabling GPU in Google Colab**
Before we start, you will need access to a GPU.  Fortunately, Google Colab provides free access to computing resources including GPUs. The GPUs currently available in Colab include Nvidia K80s, T4s, P4s and P100s. Unfortunately, there is no way to choose what type of GPU you can connect to in Colab. [See here for information](https://research.google.com/colaboratory/faq.html#gpu-availability).

To enable GPU in Google Colab:
1. Navigate to Edit→Notebook Settings or Runtime→Change Runtime Type.
2. Select GPU from the Hardware Accelerator drop-down.

### **Other dependencies**

We'll also take a moment to install and import the necessary dependencies for executing the rest of the notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install codecarbon
# pip install dash
# pip install dash-bootstrap-components==0.13.1
# pip install netCDF4
# pip install fire
# pip install optuna
# 
# mkdir code_carbon

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import random

import numpy as np
import optuna
import pandas as pd
import scipy.stats
import sklearn
import sklearn.ensemble
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import xarray as xr
from codecarbon import EmissionsTracker
from google.colab.output import eval_js
from matplotlib import pyplot as plt
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# Set seeds for reproducibility
# Per PyTorch, results may not be reproducible between CPU and GPU executions,
# even when using identical seeds
random.seed(0)
torch.manual_seed(0)
np.random.seed(0)

"""<a name="climate-impact"></a>
# **1. Climate Impact of Machine Learning**

Machine learning models can have a wide-ranging and potentially subtle impact on our climate. We categorize potential emissions from ML models into three broad levels:
* System-level
* Application-related
* Computing-related

This tutorial will focus only on the computing-related impact of ML models but we'll discuss each of these categories to capture the complete picture of machine learning’s role in the climate crisis.

<table style="text-align: center;">
<tr>
  <td>
   <img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41558-022-01377-7/MediaObjects/41558_2022_1377_Fig1_HTML.png"/>
  </td>
</tr>
<tr>
  <th>Fig. 1 from [1]</a></th>
</tr>
</table>

## **1.1 System-level impact**

System-level impacts consider the societal implications of ML. These types of impacts may have the greatest potential for GHG emissions. For example, implementing a recommender system that increases the demand for physical goods also increases the emissions from the manufacture and eventual disposal of these products. Shifting to autonomous cars could reduce demand for public transit and simultaneously increase emissions&mdash;through the material extraction and car manufacturing process and from operating the vehicles if the cars aren’t electrified.

## **1.2 Application-related impact**

Application-related impacts consider _how_ a model will be used in the wider context. Does the model enable carbon emitters or hinder the expansion of renewable energy? For example, machine learning has been used to assist in identifying new oil fields for exploration [2]. AI has also helped to identify and track livestock in factory farms, an industry that accounts for about 9% of global GHG emissions [3].

Depending on the scenario, these application-related impacts can far exceed the emissions from hardware and computing. Of course, ML can also be used to _reduce_ GHG emissions. For some hands-on examples of these types of situations, check out [CCAI’s other tutorials](https://www.climatechange.ai/tutorials?).

## 1.3 Computing-related impact

The computing-related climate impact of ML models can be divided into _embodied_ and _operational_ emissions. Embodied emissions represent the climate impact of the hardware involved in machine learning, such as material extraction and manufacture. Operational emissions are those associated with creating and productionalizing an ML model, including data storage and processing, model training, and model inference.

<table style="text-align: center;" width="50%">
<tr>
  <td>
   <img src="data:image/png;base64,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" width="100%">
  </td>
</tr>
<tr>
  <th>The ML Lifecycle</a></th>
</tr>
</table>

> **Quick note**: The ML terms _"productionalizing"_, _"operationalizing"_, or _"deploying"_ a model all refer to the process of providing the model for other users to make predictions. The term _"model inference"_ is the stage of the machine learning lifecycle when we make predictions, which is also referred to as _"scoring"_ new data.

While it’s difficult to estimate the overall contribution of ML to the climate crisis, the field of Information and Communications Technology (ICT) accounts for about 1.4% of global GHG emissions. Roughly one-third of these emissions are related to the manufacture and handling of physical materials [1].

### **1.3.1 Case study: ChatGPT** 🤖

While ICT's contribution to global emissions may seem small, the amount of compute needed to achieve state-of-the-art ML performance has historically doubled every 3.4 months [4]. These energy increases could be offset by hardware and algorithmic improvements [5]. But given the latest trend toward developing ever larger and more complex models, such as ChatGPT, we should remain vigilant in tracking and reducing the emissions associated with machine learning models.

With that in mind, let's take a closer look at the emissions involved with both training and using ChatGPT!

Training ChatGPT’s underlying language model, GPT-3, consumed an estimated 1,287 MWh [5]. To put this number in perspective, the average annual electricity consumption for an American in 2021 was 10,632 kWh [10]. That means the energy used to train GPT-3 was the equivalent of adding 121 additional Americans to the grid for one year.

Of course, we only realize value from GPT-3 when it's used by the world. ChatGPT saw more than 590 million unique visits in January 2023 alone [9]. When we calculate a model’s carbon impact, we must consider all aspects of the ML lifecycle.

Based on some [back-of-the-envelope calculations](https://towardsdatascience.com/chatgpts-electricity-consumption-7873483feac4) by Kasper Groes Albin Ludvigsen, ChatGPT’s electricity consumption in January 2023 is estimated to have been between 1,168 MWh and 23,364 MWh&mdash;roughly 1 and 18 times the amount of energy required to initially train the model. And that’s the energy estimate for just one month!

As you integrate machine learning into your own research, you will likely train models that are far smaller than GPT-3. But that doesn’t mean that your work won’t have a climate impact. Training a useful model requires a lot of experimentation, and the energy consumed by all those iterations can add up.

To reduce emissions from machine learning, we must first track them! Even if you're unable to apply any of the suggested strategies from this tutorial to minimize your impact, simply logging and reporting the associated emissions from your ML project is an important step.

Let's learn how we can calculate these emissions in the next section.

<a name="methodology"></a>
# **2. Methodology: Estimating Emissions from ML**

A model’s carbon footprint from computing-related impacts can be estimated as the product of electricity consumed and the carbon intensity of that electricity. To reduce the climate impact of a model, we can either reduce the amount of electricity required or shift our compute to an energy grid with a lower carbon intensity.

## **2.1 Introducing CodeCarbon**

To measure the computing-related carbon footprint of ML training, we’ll use a Python package called [CodeCarbon](https://github.com/mlco2/codecarbon). CodeCarbon was first developed in 2020 by a team from [MILA](https://mila.quebec/en/), [BCG GAMMA](https://www.bcg.com/beyond-consulting/bcg-gamma/overview), [Haverford College](https://www.haverford.edu/), and [Comet](https://www.comet.com/site/).

CodeCarbon uses the following formula to calculate the carbon footprint of executing your code:

<center><p><p> <img src="https://github.com/mlco2/codecarbon/raw/master/docs/edit/images/calculation.png" alt="alt" width="75%"/></center>

There are several other packages that provide similar functionality, such as [carbontracker](https://github.com/lfwa/carbontracker), [experiment-impact-tracker](https://github.com/Breakend/experiment-impact-tracker), and [eco2ai](https://github.com/sb-ai-lab/Eco2AI). However, CodeCarbon remains the most continuously updated and supported codebase among these emissions trackers.

If you're interested in learning more about the differences among these open-source trackers, please see Table 1 in [6].

## **2.2 Initializing the CodeCarbon tracker**

Let's start using CodeCarbon!

Note that CodeCarbon is not limited to measuring the impact of machine learning code. You can instantiate the tracker at any time to measure emissions from executing any type of code.

If you're more familiar with Python, you can also add the CodeCarbon tracker to your code as a context manager or as a decorator. Check out the [CodeCarbon documentation](https://mlco2.github.io/codecarbon/usage.html#) for more information on these techniques.
"""

# Instantiate the tracker object
tracker = EmissionsTracker(
    output_dir="./code_carbon/",  # define the directory to which we'll write our emissions results
    output_file="emissions.csv",  # define the name of the file containing our emissions results
    # log_level='error' # comment out this line to see regular output
)
tracker.start()

"""## **2.3 Measuring Power Usage**

First, let's define some commonly used hardware in the field of machine learning:
- **CPU:** Central Processing Unit
  - Manages all general-purpose programming.
- **GPU:** Graphical Processing Unit
  - Enhances computer graphics and accelerates machine learning workloads.
- **TPU:** Tensor Processing Unit
  - Google's custom-built processors to accelerate TensorFlow projects. [TensorFlow](https://www.tensorflow.org/) is a popular open-source deep learning framework developed by Google.
- **RAM:** Random Access Memory
  - Temporary memory that a computer can access quickly.

The main difference between CPU's and newer processors like GPU's and TPU's is the number of cores. You can think of a "core" as the basic building block of a processor&mdash;the more cores within a processor, the more processes that hardware can execute at one time.

CPU's contain only a handful of cores, which means they can only execute a few processes simultaneously. This is known as _serial processing_.

In contrast, GPU's and TPU's have thousands of cores, which means they can break down complicated tasks into thousands (or even millions!) of separate processes and execute them at the same time. This is known as _parallel processing_.

For a more intuitive understanding of the difference between serial and parallel processing, I recommend watching the Mythbusters video below.
"""

from IPython.display import YouTubeVideo

YouTubeVideo("-P28LKWTzrI")

"""#### **2.3.1 Hardware Power Consumption**

Hardware power consumption is often measured in Thermal Design Power (TDP), which represents the maximum amount of heat a CPU or GPU will generate under an intense workload.

Processors that specialize in parallel processing typically consume more power than CPU's. For example, the median TDP of the [CPU list maintained by CodeCarbon](https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/hardware/cpu_power.csv) is 65 W. Meanwhile, the TDP of GPU's usually exceeds 100 W.

This does not mean that leveraging a GPU to train your machine learning model necessarily results in greater emissions. Thanks to parallel processing, GPU's can dramatically shorten the time needed to train a large model and reduce the overall power load from the task.

The question of which kind of hardware is "greener" depends on the situation. Research has found that there is likely a specific optimal number of cores we could leverage in a given parallel computation to minimize emissions, as illustrated in the plot below [11].

<center><p><p> <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFhKxp5JhjA_RQ_hIo1pHQ.png" alt="alt" width="60%"/></center>

### **2.3.2 CodeCarbon Hardware Tracking**

CodeCarbon measures the electricity consumption of the GPU, CPU, and RAM used for executing your code (note that as of April 2023, [TPU’s are not supported](https://github.com/mlco2/codecarbon/issues/116) due to the difficulty of obtaining live power usage for this type of hardware). These measurements are taken at regular time intervals, with a default setting of 15 seconds.

Let's see what hardware CodeCarbon has found running under the hood of this Colab notebook. The last 5 lines of the output from when we instantiated the `tracker` object are the most meaningful. Here is what was displayed to me when I ran this notebook in April 2023:
```
[codecarbon INFO @ 13:07:27]   Available RAM : 12.681 GB
[codecarbon INFO @ 13:07:27]   CPU count: 2
[codecarbon INFO @ 13:07:27]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz
[codecarbon INFO @ 13:07:27]   GPU count: 1
[codecarbon INFO @ 13:07:27]   GPU model: 1 x Tesla T4
```

CodeCarbon has assessed the memory of my RAM (12.681 GB) and assumes that each 8 GB of memory accounts for 3 W of power. Therefore, CodeCarbon will multiply our RAM's memory by 3/8 to determine the power consumption in watts. In our case, that comes out to about 4.76 W.

CodeCarbon has also counted two Intel(R) Xeon(R) CPU's. You might notice a WARNING in your output that reads:
```
[codecarbon WARNING @ 13:07:25] No CPU tracking mode found. Falling back on CPU constant mode.
```
This means that Colab does not expose enough information about the available CPU's for CodeCarbon to accurately assess their power consumption. To compensate, CodeCarbon will assign a constant TDP of 85 W for all CPU power expenditure and assumes that only half of that TDP is consumed on average. Therefore the final power assigned to our CPU's is 42.5 W.

Finally, we note that CodeCarbon has found one Nvidia Tesla T4 GPU. CodeCarbon leverages Nvidia's [`pynvml`](https://pypi.org/project/pynvml/) library to directly access the GPU's power rating.
"""

# Note that CodeCarbon will output the energy consumption every time you execute code
# Comment out the `log_level` argument when instantiating the tracker to suppress this output
print("All hardware identified:", tracker._hardware)
print("GPU Power:", tracker._hardware[1].total_power())

"""## **2.4 Measuring Carbon Intensity**

CodeCarbon calculates a weighted average of the emissions from the energy sources that make up your local grid (or the grid used by your cloud provider). When available, CodeCarbon uses the global carbon intensity of electricity per [cloud provider](https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/cloud/impact.csv) or per [country](https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/private_infra/eu-carbon-intensity-electricity.csv).  

If the carbon intensity of a particular country is unavailable, CodeCarbon may still have access to its electricity mix and will use the [carbon intensities of each energy source](https://mlco2.github.io/codecarbon/methodology.html#id4) to calculate the weighted average. Note that the package assigns the same carbon intensity to an energy source regardless of where the energy is produced (i.e. coal's carbon intensity is considered to be the same in both West Virginia and Germany).

If neither the country’s carbon intensity or its energy mix is available, CodeCarbon applies the world average of 475 g CO<sub>2</sub>eq/kWh. Note that "CO<sub>2</sub>eq" stands for "carbon dioxide equivalents" and represents the global warming potential of various greenhouse gases.

> **IMPORTANT NOTE:** Given the simplifying assumptions on an energy grid's carbon intensity listed above, the emissions returned by CodeCarbon are only an estimate!

### 2.4.1 Colab Compute Region

Let's determine what compute region Colab has selected.
"""

tracker._geo

"""Colab has connected to Google's data center in Iowa to execute my code. Note that Colab might connect to a different region every time you restart your runtime. For example, I'm physically located in Chicago but Colab often connects me to Singapore's data center!

[CodeCarbon's data](https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/private_infra/2016/usa_emissions.json) estimates that Iowa's energy grid has an energy intensity of about 998 lbs CO<sub>2</sub>eq per MWh as of 2016. Unfortunately, CodeCarbon does not have more recent estimates of energy intensity for regions in the USA or Canada.

A quick scan of CodeCarbon's 2016 [USA energy intensities](https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/private_infra/2016/usa_emissions.json) shows that Iowa falls somewhere in the middle of U.S. states. If we wanted to optimize our energy efficiency, we might restart our runtime until we connected to Vermont (57 lbs CO<sub>2</sub>eq/MWh) or Idaho (189 lbs CO<sub>2</sub>eq/MWh).

This kind of energy efficiency optimization is much easier for cloud environments such as AWS, GCP, or Azure that allow you to choose which compute region you'd like to connect to.

## **2.5 CodeCarbon Dashboard**

Stopping the tracker will total up all the energy consumed by this hardware since the tracker's initialization and multiply by our grid's energy intensity to estimate emissions in CO<sub>2</sub>eq.

> **IMPORTANT NOTE**: Since it is unlikely that Colab will provision you with the same underlying hardware and connect you to the same compute region as when I ran this notebook, your emissions from running this code will probably differ from those currently displayed in the tutorial!

Once stopped, the tracker will create a new file (in this case titled `emissions.csv`) in our notebook's working directory with info on the emissions generated from this experiment
"""

emissions = tracker.stop()
print(f"Emissions from this training run: {emissions:5f} kg CO2eq")

"""We can also contextualize these emissions via CodeCarbon's dashboard.

This dashboard is hosted via a Dash app. In order to access this app on Colab's servers, we'll need a proxy link. After executing the cell below, click on the link next to the text stating "CLICK HERE". Once you're done reviewing the dashboard, click on the cell's stop button ⏹.
"""

## UNCOMMENT THIS CELL TO RUN THE CODECARBON DASHBOARD ##

# # Display the proxy link
# print("CLICK HERE:", eval_js("google.colab.kernel.proxyPort(8050)"), "\n")

# # Point to our emissions csv file
# !carbonboard --filepath="code_carbon/emissions.csv" --port=8050

"""When I ran this notebook in April 2023, executing just a few cells of code to assess our underlying hardware and compute region resulted in carbon emissions equivalent to watching 7 minutes of a 32-inch LCD TV.

<a name="codecarbon-experiments"></a>
# **3. CodeCarbon Experiments**

Let's try tracking our carbon emissions while training a machine learning model. We'll also explore a few techniques to train in a more efficient way and reduce our emissions.

## **3.1 Generate Training and Test Data**

We'll use the same data as the [Forecasting the El Nino / Southern Oscillation with Machine Learning tutorial](https://colab.research.google.com/drive/1nOL7rB8EnFTvLoarerVOtECYBwJKth-t?usp=sharing) from earlier in the Summer School program. Remember that the goal of that tutorial was to leverage historical sea surface temperatures to correctly forecast the Nino3.4 index, which indicates the state of El Nino / Southern Oscillation (ENSO).

We'll move quickly through this data preparation section as the focus of this tutorial is on model training and emissions tracking. For a more detailed walkthrough, please reference the original forecasting tutorial.
"""

# Download Nino3.4 index (this index measures the state of ENSO by encoding the
# average temperature anomaly in the equatorial Pacific)
!gdown --id 1aGvitA8rYrHRDxNd2XD4AAFsahCQsv0t

# Download sst observations from 1880 to 2018
!gdown 1-xefk3imP4Q-8GevIV2YIo82iP5rTdSc

def load_enso_indices():
    """
    Reads in the txt data file to output a pandas Series of ENSO vals

    Returns:
      pd.Series: monthly ENSO values starting from 1870-01-01
    """
    with open("nino34.long.anom.data.txt") as f:
        line = f.readline()
        enso_vals = []
        while line:
            yearly_enso_vals = map(float, line.split()[1:])
            enso_vals.extend(yearly_enso_vals)
            line = f.readline()

    enso_vals = pd.Series(enso_vals)
    enso_vals.index = pd.date_range("1870-01-01", freq="MS", periods=len(enso_vals))
    enso_vals.index = pd.to_datetime(enso_vals.index)
    return enso_vals


def assemble_basic_predictors_predictands(start_date, end_date, lead_time):
    """
    Creates a tuple of the predictors (np array of sst temperature anomalies)
    and the predictands (np array the ENSO index at the specified lead time).

    Parameters:
        start_date (str): the start date from which to extract sst
        end_date (str): the end date
        lead_time (str): the number of months between each sst
            value and the target Nino3.4 Index

    Returns:
        (np.array, np.array)

    """
    ds = xr.open_dataset("sst.mon.mean.trefadj.anom.1880to2018.nc")
    sst = ds["sst"].sel(time=slice(start_date, end_date))
    num_time_steps = sst.shape[0]

    # sst is a 3D array: (time_steps, lat, lon)
    # in this tutorial, we will not be using ML models that take
    # advantage of the spatial nature of global temperature
    # therefore, we reshape sst into a 2D array: (time_steps, lat*lon)
    # (At each time step, there are lat*lon predictors)
    sst = sst.values.reshape(num_time_steps, -1)
    sst[np.isnan(sst)] = 0

    X = sst

    start_date_plus_lead = pd.to_datetime(start_date) + pd.DateOffset(months=lead_time)
    end_date_plus_lead = pd.to_datetime(end_date) + pd.DateOffset(months=lead_time)
    y = load_enso_indices()[slice(start_date_plus_lead, end_date_plus_lead)]

    ds.close()
    return X, y


def plot_nino_time_series(y, predictions, title):
    """
    Plots the ENSO time series against the predictions.

    Parameters:
      y (pd.Series): time series of the true Nino index
      predictions (np.array): time series of the predicted Nino index (same
          length and time as y)
      title (str): the title of the plot

    Returns:
      (None): Displays the plot
    """
    predictions = pd.Series(predictions, index=y.index)
    predictions = predictions.sort_index()
    y = y.sort_index()

    plt.plot(y, label="Ground Truth")
    plt.plot(predictions, "--", label="ML Predictions")
    plt.legend(loc="best")
    plt.title(title)
    plt.ylabel("Nino3.4 Index")
    plt.xlabel("Date")
    plt.show()
    plt.close()

"""## **3.2 Algorithm Choice**

One of the easiest ways to reduce emissions is by choosing a simpler algorithm. We often don't need to immediately reach for a complicated neural network when a more traditional machine learning method would suffice.

<table style="text-align: center;" width="50%">
<tr>
  <td>
   <img src="https://external-preview.redd.it/khzQQ3xzswIW4HGIvesRWJXvCbnvdWNarIyU4-GVl2I.jpg?width=640&crop=smart&auto=webp&s=180b2912c6bef455fcb4ef7565dd5093edbabe23" width="100%">
  </td>
</tr>
<tr>
  <th>Applying a neural network when linear regression would do.</a></th>
</tr>
</table>

We'll discuss more about the potential trade-offs involved in choosing a simple algorithm over a more complicated one at the end of this section. For now, we'll focus on training two different models and capturing the emissions from each one.

### **3.2.1 Training a Baseline Model: Linear Regression**

Let's first attempt to model ENSO using linear regression. We'll track the emissions associated with training this simple model.
"""

experiment_name = "linear_regression"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

# Load train, val, and test sets
X_train, y_train = assemble_basic_predictors_predictands(
    "1980-01-01", "1995-12-31", lead_time=1
)
X_val, y_val = assemble_basic_predictors_predictands(
    "1997-01-01", "2006-12-31", lead_time=1
)
X_test, y_test = assemble_basic_predictors_predictands(
    "2007-01-01", "2017-12-31", lead_time=1
)

# Train a linear regression model
regr = sklearn.linear_model.LinearRegression()
regr.fit(X_train, y_train)

predictions = regr.predict(X_val)
corr, _ = scipy.stats.pearsonr(predictions, y_val)
baseline_rmse = mean_squared_error(y_val, predictions, squared=False)
print("RMSE: {:.2f}".format(baseline_rmse))

# Plot predictions against actuals
plot_nino_time_series(
    y_val,
    predictions,
    "Predicted and True Nino3.4 Indices on Training Set at 1 Month Lead Time. \n Corr: {:.2f}".format(
        corr
    ),
)

# Evaluate emissions
baseline_emissions = tracker.stop()
print(f"Emissions from this training run: {baseline_emissions:.5f} kg CO2eq")

"""### **3.2.2 Training a CNN**

Let's compare the emissions and performance of a linear regression model to a more complex convolutional neural network (CNN).

First, we'll define some necessary data wrangling code.
"""

def assemble_predictors_predictands(start_date, end_date, lead_time,
                                    num_input_time_steps=1):
    """
    Creates a tuple of the predictors (np array of sst temperature anomalies)
    and the predictands (np array the ENSO index at the specified lead time).

    Parameters:
      start_date (str): the start date from which to extract sst
      end_date (str): the end date
      lead_time (int): the number of months between each sst value and the
          target Nino3.4 Index
      num_input_time_steps (int): the number of time steps to use for each
          predictor sample

    Returns:
      (np.array, np.array)

    """
    file_name = "sst.mon.mean.trefadj.anom.1880to2018.nc"
    variable_name = "sst"
    ds = xr.open_dataset(file_name)
    sst = ds[variable_name].sel(time=slice(start_date, end_date))

    num_samples = sst.shape[0]
    # sst is a (num_samples, lat, lon) array
    # the line below converts it to (num_samples, num_input_time_steps, lat, lon)
    sst = np.stack(
        [
            sst.values[n - num_input_time_steps : n]
            for n in range(num_input_time_steps, num_samples + 1)
        ]
    )
    num_samples = sst.shape[0]
    sst[np.isnan(sst)] = 0
    X = sst

    start_date_plus_lead = pd.to_datetime(start_date) + pd.DateOffset(
        months=lead_time + num_input_time_steps - 1
    )
    end_date_plus_lead = pd.to_datetime(end_date) + pd.DateOffset(months=lead_time)
    y = load_enso_indices()[slice(start_date_plus_lead, end_date_plus_lead)]
    ds.close()
    return X.astype(np.float32), y.astype(np.float32)


class ENSODataset(Dataset):
    def __init__(self, predictors, predictands):
        self.predictors = predictors
        self.predictands = predictands
        assert (
            self.predictors.shape[0] == self.predictands.shape[0]
        ), "The number of predictors must equal the number of predictands!"

    def __len__(self):
        return self.predictors.shape[0]

    def __getitem__(self, idx):
        return self.predictors[idx], self.predictands[idx]

"""Now we'll define the architecture of our convolutional neural network."""

class CNN(nn.Module):
    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):
        """
        Constructs all the necessary attributes for the CNN class.

        Parameters:
          num_input_time_steps (int) : the number of input time steps in the predictor
          print_feature_dimension (bool) : whether or not to print out the dimension
              of the features extracted from the conv layers
        """
        super().__init__()
        self.conv1 = nn.Conv2d(num_input_time_steps, 6, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.print_layer = Print()

        # TIP: print out the dimension of the extracted features from
        # the conv layers for setting the dimension of the linear layer!
        # Using the print_layer, we find that the dimensions are
        # (batch_size, 16, 42, 87)
        self.fc1 = nn.Linear(16 * 42 * 87, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 1)
        self.print_feature_dimension = print_feature_dimension

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        if self.print_feature_dimension:
            x = self.print_layer(x)
        x = x.view(-1, 16 * 42 * 87)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


class Print(nn.Module):
    """
    This class prints out the size of the features
    """

    def forward(self, x):
        print(x.size())
        return x

"""Let's define a class to train our CNN."""

class TrainCNN:
    def __init__(self, trainloader, testloader, net, criterion, optimizer):
        """
        Contains the necessary code for training a CNN.

        Parameters
          trainloader (torch.utils.data.DataLoader): PyTorch dataloader that loads the
              predictors and predictands for the train dataset
          testloader (torch.utils.data.DataLoader): PyTorch dataloader that loads the
              predictors and predictands for the test dataset
          net (nn.Module): Neural network architecture
          criterion (nn): Loss function (i.e. root mean squared error)
          optimizer (torch.optim): Optimizer to use update the neural network
              architecture to minimize the loss function
        """
        self.trainloader = trainloader
        self.testloader = testloader
        self.net = net
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.train_losses = []
        self.test_losses = []
        self.best_loss = np.infty

    def _calculate_loss(self, data):
        """
        Calculates the loss of the current model.

        Parameters:
          data (torch.Tensor): Predictors and predictands against which to
              calculate loss of the current model

        Returns:
            torch.Tensor
        """
        # Get a mini-batch of predictors and predictands
        batch_predictors, batch_predictands = data
        batch_predictands = batch_predictands.to(self.device)
        batch_predictors = batch_predictors.to(self.device)

        # Zero the parameter gradients
        self.optimizer.zero_grad()

        # Calculate the predictions of the current neural network
        predictions = self.net(batch_predictors).squeeze(1)

        # Quantify the quality of the predictions using a
        # loss function (aka criterion) that is differentiable
        loss = self.criterion(predictions, batch_predictands)
        return loss

    def _train(self):
        """
        Trains the CNN for one iteration.

        Returns:
            None: Appends the current training loss
        """
        running_loss = 0.0
        self.net.train()

        for i, data in enumerate(self.trainloader):
            loss = self._calculate_loss(data)

            # The backward pass: calculates the gradients of each weight
            # of the neural network with respect to the loss
            loss.backward()

            # The optimizer updates the weights of the neural network
            # based on the gradients calculated above and the choice
            # of optimization algorithm
            self.optimizer.step()
            running_loss += loss.item()

        self.train_losses.append(running_loss / len(self.trainloader))

    def _evaluate_epoch(self):
        """
        Evaluates the test loss at the current epoch and updates the best loss
        if current loss is less than the previous best loss.

        Return:
          None: Appends the current test loss
        """
        running_loss = 0.0
        self.net.eval()

        for i, data in enumerate(self.testloader):
            loss = self._calculate_loss(data)
            running_loss += loss.item()

        # Save the model weights that have the best performance!
        if running_loss < self.best_loss:
            self.best_loss = running_loss
            torch.save(self.net, "current_best_cnn.pt")

        self.test_losses.append(running_loss / len(self.testloader))

    def train_network(self, num_epochs=40, verbose=True):
        """
        Trains a neural network for a specified number of epochs.

        Parameters:
          num_epochs (int): Number of epochs for which to train the network
          verbose (bool): Whether or not to print the current train and test loss

        Returns:
          None
        """
        net = self.net.to(self.device)
        for epoch in range(num_epochs):
            self._train()
            self._evaluate_epoch()
            if verbose:
                print(
                    "train Set: Epoch {:02d}. loss: {:3f}".format(
                        epoch + 1, self.train_losses[-1]
                    )
                )
                print(
                    "test Set: Epoch {:02d}. loss: {:3f}".format(
                        epoch + 1, self.test_losses[-1]
                    )
                )

        self.net = torch.load("current_best_cnn.pt")
        self.net.eval()
        self.net.to(self.device)

    def evaluate_best_model(self):
        """
        Makes predictions against the current best model.

        Returns:
          np.array: Predictions from the model
        """
        if self.best_loss == np.infty:
            raise Exception("The model has not yet been trained!")

        predictions = np.asarray([])
        for i, data in enumerate(self.testloader):
            batch_predictors, batch_predictands = data
            batch_predictands = batch_predictands.to(self.device)
            batch_predictors = batch_predictors.to(self.device)

            batch_predictions = self.net(batch_predictors).squeeze()
            # Edge case: if there is 1 item in the batch, batch_predictions becomes a float
            # not a Tensor. the if statement below converts it to a Tensor
            # so that it is compatible with np.concatenate
            if len(batch_predictions.size()) == 0:
                batch_predictions = torch.Tensor([batch_predictions])
            predictions = np.concatenate(
                [predictions, batch_predictions.detach().cpu().numpy()]
            )
        return predictions

"""Now we'll kick off a new CodeCarbon experiment to track emissions from training this simple CNN."""

experiment_name = "cnn"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

# Assemble numpy arrays corresponding to predictors and predictands
train_start_date = "1960-01-01"
train_end_date = "2005-12-31"
num_input_time_steps = 1
lead_time = 1

train_predictors, train_predictands = assemble_predictors_predictands(
    train_start_date,
    train_end_date,
    lead_time,
    num_input_time_steps=num_input_time_steps,
)
test_predictors, test_predictands = assemble_predictors_predictands(
    "2007-01-01",
    "2017-12-31",
    lead_time,
    num_input_time_steps=num_input_time_steps,
)

# Convert the numpy ararys into ENSODataset, which is a subset of the
# torch.utils.data.Dataset class.  This class is compatible with
# the torch dataloader, which allows for data loading for a CNN
train_dataset = ENSODataset(train_predictors, train_predictands)
test_dataset = ENSODataset(test_predictors, test_predictands)

# Create a torch.utils.data.DataLoader from the ENSODatasets() created earlier!
trainloader = DataLoader(train_dataset, batch_size=10)
testloader = DataLoader(test_dataset, batch_size=10)

# Train the model
net = CNN(num_input_time_steps=num_input_time_steps)
optimizer = optim.Adam(net.parameters(), lr=0.0001)
cnn_train_session = TrainCNN(
    trainloader, testloader, net, criterion=nn.MSELoss(), optimizer=optimizer
)
cnn_train_session.train_network()

# Evaluate predictions from the best performing model
predictions = cnn_train_session.evaluate_best_model()

# Plot predictions against actuals over time
corr, _ = pearsonr(test_predictands, predictions)
cnn_rmse = mean_squared_error(test_predictands, predictions, squared=False)
plot_nino_time_series(
    test_predictands,
    predictions,
    "Predictions. Corr: {:3f}. RMSE: {:3f}.".format(corr, cnn_rmse),
)

# Stopping the tracker will create a new file (emissions.csv) in our notebook's
# working directory with info on the emissions generated from this experiment
cnn_emissions = tracker.stop()
print(f"Emissions from this training run: {cnn_emissions:5f} kg CO2eq")

"""### **3.2.3 Comparing Emissions and Accuracy**

Let's compare the emissions from training a linear regression model with those from training a CNN.
"""

print(
    f"CNN emissions account for {cnn_emissions/baseline_emissions:.1f} times the emissions from linear regression."
)

"""Wow! Linear regression is the clear environmental winner in terms of reducing carbon impact. Of course, we must also consider the performance of our model. How well does it forecast ENSO? Let's compare the root squared mean error (RMSE) from both models. Remember that a lower RMSE indicates a better fit."""

print(f"Previous baseline RMSE: {baseline_rmse:5f}")
print(f"Current CNN RMSE: {cnn_rmse:5f}")

"""As expected, those extra emissions from training the CNN resulted in a performance boost. Research indicates that ML model quality has a diminishing power-law relationship with energy consumption ($ Q\:α \: E^{-k} $, where $Q$ is model quality and $E$ is energy consumed). However, the power of this relationship ($k$) is very small (0.002-0.004) [7]. This means we need to expend a lot more energy for marginal performance gains.

Whether or not this extra accuracy is necessary depends on your situation. For example, we'd prioritize accuracy at all costs for a model that detects lung cancer from CT scans. We wouldn't want to miss any early indications on a sick patient and conversely, we wouldn't want to cause undue stress by incorrectly diagnosing a healthy patient with cancer. On the other hand, we might not need the most accurate model when predicting which customers are likely to churn from an online retailer since the cost of being wrong is much lower than our lung cancer example.

Let's visualize the relationship between emissions and performance for these two models.
"""

def plot_comparison(results):
    """
    Plots GHG emissions against RMSE.
    """
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.bar(results["Category"], results["Emissions"], label="Emissions (g CO2eq)")
    ax.set_ylabel("Emissions (g CO2eq)")

    ax2 = ax.twinx()
    ax2.scatter(
        results["Category"], results["RMSE"], color="orange", label="RMSE",
    )
    ax2.set_ylabel("RMSE")
    ax2.set_ylim([0, max(results["RMSE"]) * 1.2])

    # ask matplotlib for the plotted objects and their labels
    lines, labels = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax2.legend(
        lines + lines2, labels + labels2, loc="center left", bbox_to_anchor=(1.1, 0.9)
    );

model_results = pd.DataFrame(
    {
        "Category": ["Linear Regression", "CNN"],
        "RMSE": [baseline_rmse, cnn_rmse],
        "Emissions": [baseline_emissions * 1000, cnn_emissions * 1000],
    }
)

plot_comparison(model_results)

"""### Exercise 1

⭐ **YOUR TURN!:** Try reducing the size of the CNN by removing the second linear layer within the CNNReduced class in the cell below. How does this change affect the model's performance and efficiency in terms of emissions?
"""

class CNNReduced(CNN):
    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):
        """
        Constructs all the necessary attributes for a CNN class.

        Parameters:
          num_input_time_steps (int) : the number of input time steps in the predictor
          print_feature_dimension (bool) : whether or not to print out the dimension
              of the features extracted from the conv layers
        """
        # this class inherits its attributes from the CNN class above!
        super().__init__(num_input_time_steps, print_feature_dimension)

        # TIP: print out the dimension of the extracted features from
        # the conv layers for setting the dimension of the linear layer!
        # Using the print_layer, we find that the dimensions are
        # (batch_size, 16, 42, 87)

        # Part 1
        self.fc1 = ### YOUR CODE HERE ###
        self.fc2 = ### YOUR CODE HERE ###

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        if self.print_feature_dimension:
            x = self.print_layer(x)
        x = x.view(-1, 16 * 42 * 87)
        # Part 2
        x = ### YOUR CODE HERE ###
        x = ### YOUR CODE HERE ###
        return x

experiment_name = "cnn_reduced"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

# Train the reduced model with the same number of input time steps (num_input_time_steps)
# as the previous CNN

# Part 3
net = ### YOUR CODE HERE ###
optimizer = optim.Adam(net.parameters(), lr=0.0001)
cnn_train_session_reduced = TrainCNN(
    trainloader, testloader, net, criterion=nn.MSELoss(), optimizer=optimizer
)
cnn_train_session_reduced.train_network()

# Evaluate predictions from the best performing model
predictions_cnn_reduced = cnn_train_session_reduced.evaluate_best_model()

"""Let's evaluate the results from the reduced CNN."""

cnn_reduced_rmse = mean_squared_error(test_predictands, predictions_cnn_reduced, squared=False)

print(f"Previous CNN RMSE: {cnn_rmse:5f}")
print(f"Reduced CNN RMSE: {cnn_reduced_rmse:5f}")

cnn_reduced_emissions = tracker.stop()
print(f"Emissions from this training run: {cnn_reduced_emissions:5f} kg CO2eq")

model_results = pd.DataFrame(
    {
        "Category": ["Original CNN", "Reduced CNN"],
        "RMSE": [cnn_rmse, cnn_reduced_rmse],
        "Emissions": [cnn_emissions * 1000, cnn_reduced_emissions * 1000],
    }
)

plot_comparison(model_results)

print(
    f"What is the percentage that emissions were reduced by removing the second layer of the CNN?\n{(cnn_emissions - cnn_reduced_emissions)/cnn_emissions:.1%}"
)
print('')
print(
    f"How much did error increase by removing the second layer?\n{abs(cnn_rmse - cnn_reduced_rmse)/cnn_rmse:.1%}"
)

"""#### Solution"""

# Part 1
self.fc1 = nn.Linear(16 * 42 * 87, 120)
self.fc2 = nn.Linear(120, 1)

# Part 2
x = F.relu(self.fc1(x))
x = self.fc2(x)

# Part 3
net = CNNReduced(num_input_time_steps=num_input_time_steps)

"""You should find that emissions are reduced by 16-18% and error increases by around 10-12%.

### Exercise 2

⭐ **YOUR TURN!:** If you're more familiar with neural networks, experiment with applying early stopping to our CNN training process. An "early stopping" approach will terminate training if the model's validation loss doesn't decrease for several epochs in a row. How does reducing the number of epochs via early stopping affect the final model's performance and its emissions?
"""

class TrainCNNwithEarlyStopping(TrainCNN):
    def __init__(self, trainloader, testloader, net, criterion, optimizer,
                 patience):
        """
        Contains the necessary code for training a CNN with early stopping.

        Parameters
          trainloader (torch.utils.data.DataLoader): PyTorch dataloader that loads the
              predictors and predictands for the train dataset
          testloader (torch.utils.data.DataLoader): PyTorch dataloader that loads the
              predictors and predictands for the test dataset
          net (nn.Module): Neural network architecture
          criterion (nn): Loss function (i.e. root mean squared error)
          optimizer (torch.optim): Optimizer to use update the neural network
              architecture to minimize the loss function
          patience (int): Number of epochs without test loss improvement that we
              will tolerate
        """
        # this class inherits its attributes from the TrainCNN class above!
        super().__init__(trainloader, testloader, net, criterion, optimizer)
        # Part 1
        self.patience = ### YOUR CODE HERE ###
        self.current_patience = ### YOUR CODE HERE ###

    def _evaluate_epoch(self):
        """
        Evaluates the test loss at the current epoch and updates the best loss
        if current loss is less than the previous best loss.

        Return:
          None: Appends the current test loss
        """
        running_loss = 0.0
        self.net.eval()

        for i, data in enumerate(self.testloader):
            loss = self._calculate_loss(data)
            running_loss += loss.item()

        # Save the model weights that have the best performance!
        if running_loss < self.best_loss:
            self.best_loss = running_loss
            torch.save(self.net, "current_best_cnn.pt")
            # Part 2
            self.current_patience = ### YOUR CODE HERE ###
        else:
            # Part 3
            self.current_patience -= ### YOUR CODE HERE ###
            print(f'Test loss increased past best loss. Patience: {self.current_patience}')

        self.test_losses.append(running_loss / len(self.testloader))

    def train_network(self, num_epochs=40, verbose=True):
        """
        Trains a neural network for a specified number of epochs.

        Parameters:
          num_epochs (int): Number of epochs for which to train the network
          verbose (bool): Whether or not to print the current train and test loss

        Returns:
          None
        """
        net = self.net.to(self.device)
        for epoch in range(num_epochs):
            self._train()
            self._evaluate_epoch()
            if verbose:
                print(
                    "train Set: Epoch {:02d}. loss: {:3f}".format(
                        epoch + 1, self.train_losses[-1]
                    )
                )
                print(
                    "test Set: Epoch {:02d}. loss: {:3f}".format(
                        epoch + 1, self.test_losses[-1]
                    )
                )

            # Part 4
            if self.current_patience == 0:
                ### YOUR CODE HERE ###

        self.net = torch.load("current_best_cnn.pt")
        self.net.eval()
        self.net.to(self.device)

experiment_name = "cnn_with_early_stopping"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

# Train the model
net = CNN(num_input_time_steps=num_input_time_steps)
optimizer = optim.Adam(net.parameters(), lr=0.0001)
cnn_train_session_early_stopping = TrainCNNwithEarlyStopping(
    trainloader, testloader, net, criterion=nn.MSELoss(), optimizer=optimizer,
    patience=15
)
cnn_train_session_early_stopping.train_network()

# Evaluate predictions from the best performing model
predictions_es = cnn_train_session_early_stopping.evaluate_best_model()

cnn_emissions_es = tracker.stop()
print(f"Emissions from this training run: {cnn_emissions_es:5f} kg CO2eq")

print(
    f"Early stopping reduced emissions by {(cnn_emissions - cnn_emissions_es)/cnn_emissions:.1%}."
)

cnn_with_early_stopping_rmse = mean_squared_error(test_predictands, predictions_es, squared=False)
print('')
print(
    f"RMSE before early stopping: {cnn_rmse:.3}"
)
print(
    f"RMSE with early stopping: {cnn_with_early_stopping_rmse:.3}"
)

"""#### Solution"""

# Part 1
self.patience = patience
self.current_patience = self.patience

# Part 2
self.current_patience = self.patience

# Part 3
self.current_patience -= 1

# Part 4
if self.current_patience == 0:
    break

"""Early stopping with a patience of 15 should reduce emissions by 30-50%. RMSE will increase by about 0.03.

### **3.2.4 Tracking Emissions from Model Inference**

As noted in our [ChatGPT example](https://colab.research.google.com/drive/1tTok_j2MZtWLuvcr6p_labPOxnCPM9ZL#scrollTo=1_3_1_Case_study_ChatGPT_) above, we must also account for the emissions associated with productionalizing a model. This consideration is situation-specific. Will millions of users predict using our model multiple times a day? Or will we only need to score new data with our model once a month?

We can use CodeCarbon again to assess the emissions associated with model inference. Note that we would need to deploy CodeCarbon on the infrastructure that hosts the productionalized model for an accurate assessment of the emissions associated with scoring. We'll measure the emissions associated with inference within the Colab notebook for illustration purposes only.
"""

# First we'll measure emissions from scoring with the linear regression model
experiment_name = "linear_regression_inference"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

predictions_test_lin_reg = regr.predict(X_test)

lin_reg_inference_emissions = tracker.stop()
print(f"Emissions from this training run: {lin_reg_inference_emissions} kg CO2eq")

# Now we measure emissions from scoring with the CNN
experiment_name = "cnn_inference"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

device = "cuda:0" if torch.cuda.is_available() else "cpu"
predictions_test_cnn = cnn_train_session.net(
    torch.Tensor(test_predictors).to(device)
).squeeze()

cnn_inference_emissions = tracker.stop()
print(f"Emissions from this training run: {cnn_inference_emissions} kg CO2eq")

print(
    f"The emissions from scoring with a CNN are {cnn_inference_emissions/lin_reg_inference_emissions:.1f} times the emissions from scoring with a linear regression model."
)

"""The difference in emissions from inference between the two models is much less dramatic than that from training. One reason for this is that deep learning models train over multiple epochs. Neural networks then typically require three times the energy to train on an observation as compared to inferring on that observation [8].

Be sure to measure emissions from scoring data samples of the scale you'd expect once you've deployed your model for the most accurate estimates!

## **3.3 Hyperparameter Tuning**

Hyperparameter tuning refers to the task of adjusting the data that governs the training process itself, such as the number of layers in a neural network. ML practitioners typically tune these hyperparameters to find a combination with an optimal model performance.

Let's explore the emissions associated with tuning the hyperparameters of our CNN and experiment with some strategies to make this process more efficient.

Note that in the interest of time, we will only explore a small hyperparameter space. However, these strategies are much more effective at reducing emissions when we need to search a large number of possible hyperparameter combinations to optimize model performance.

### **3.2.1 Introducing Optuna**

We'll use the [`optuna`](https://optuna.org/) package to tune our hyperparameters. To do so, we need to define an `objective` function such as the one below.

The `objective` function takes an input parameter called `trial`, which comes from Optuna's `Trial` class. This object logs a set of selected hyperparameter values and records the value of our objective function (RMSE in our case) from each trial.

We also define the search space of each hyperparameter as a dictionary named `params`. For each hyperparameter, we specify the range of the search space with the `suggest_*` methods.

- [**suggest_int:**](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) Suggests a value for a hyperparameter of type integer.
- [**suggest_categorical:**](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_categorical) Suggests a value for a categorical hyperparameter.
-[**suggest_float:**](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float) Suggests a value for a hyperparameter of type float. Note that you can specify a log distribution by setting the parameter `log=True`.

For our hyperparameter tuning session, we'll experiment with adjusting the learning rate and the optimizer of our CNN as well as the number of input time steps used to construct our training data.

> _Quick note:_ Remember that the parameter `num_input_time_steps` refers to the number of time steps for each predictor. For example, if num_input_time_steps is set to 3, then the machine learning model will take 3 months as its input. In other words, a sample predictor will be 3 months of average temperature: Jan average temperature, Feb average temperature, and March average temperature (i.e. an array of (3, lat, lon)). The predictand will be the Niño3.4 index lead_time months after March.
>
> Previously, we had trained our CNN using a `num_input_time_steps` value of 1.
"""

def objective(trial):
    """
    Defines the objective function.

    Parameters:
      trial (optuna.Trial): Provides interfaces to Optuna's functionality

    Returns:
      (int): RMSE of current trial
    """
    # Define the hyperparameter search space
    params = {
        "num_input_time_steps": trial.suggest_int("num_input_time_steps", 1, 3),
        "learning_rate": trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True),
        "optimizer": trial.suggest_categorical("optimizer", ["Adam", "SGD"]),
    }

    # Instantiate the model
    net = CNN(num_input_time_steps=params["num_input_time_steps"])

    # Create new train and test sets based on the selected num_input_time_steps parameter
    train_predictors, train_predictands = assemble_predictors_predictands(
        start_date=train_start_date,
        end_date=train_end_date,
        lead_time=1,
        num_input_time_steps=params["num_input_time_steps"],
    )
    test_predictors, test_predictands = assemble_predictors_predictands(
        start_date="2007-01-01",
        end_date="2017-12-31",
        lead_time=1,
        num_input_time_steps=params["num_input_time_steps"],
    )

    train_dataset = ENSODataset(train_predictors, train_predictands)
    test_dataset = ENSODataset(test_predictors, test_predictands)

    trainloader = DataLoader(train_dataset, batch_size=10)
    testloader = DataLoader(test_dataset, batch_size=10)

    # Specify criterion and optimizer
    criterion = torch.nn.MSELoss()
    optimizer = getattr(optim, params["optimizer"])(
        net.parameters(), lr=params["learning_rate"]
    )

    # Train a CNN using the specified hyperparameters and calculate RMSE
    cnn_train_session = TrainCNN(trainloader, testloader, net, criterion, optimizer)
    cnn_train_session.train_network(verbose=False)
    predictions = cnn_train_session.evaluate_best_model()
    rmse = mean_squared_error(test_predictands, predictions, squared=False)
    return rmse

"""### **3.3.2 TPE Search**

You might already be familiar with tuning hyperparameters via grid search in which we try every combination of hyperparameters within the space. Grid search is inherently inefficient and should be avoided in favor of more intelligent search algorithms.

One such algorithm is the Tree-structured Parzen Estimator (TPE) algorithm, which is the default hyperparameter sampling method in Optuna. On each trial for each hyperparameter, TPE fits a Gaussian Mixture Model (GMM) $l(x)$ to the hyperparameter sets that return the smallest values for the objective function.

> _Quick note:_ A Gaussian mixture model is a probabilistic model that represents normally distributed subpopulations (in this case, our hyperparameters) within an overall population. Please see this [short introduction](https://brilliant.org/wiki/gaussian-mixture-model/) for a deeper explanation.

TPE fits another GMM $g(x)$ to the remaining hyperparameter values. The hyperparameter value $x$ that maximizes the ratio $l(x)/g(x)$ would then be evaluated using the objective function. I recommend this [blog post](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f#) for more information about Bayesian-based hyperparameter tuning strategies.

TPE will run for as many trials as we specify. Let's limit ourselves to 12 trials to ensure we're not waiting too long for a result.
"""

experiment_name = "tpe_search"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

"""The TPE algorithm cannot operate on a "cold start"&mdash;in other words, TPE needs the results of some "startup" trials to fit the GMM $l(x)$. We set the parameter `n_startup_trials` to specify the number of trials that will initially leverage random sampling instead of TPE. The default value for this parameter is 10 but since we're only searching over 12 trials in total, we'll reduce this number to a lower value. Let's try 5."""

tpe_search_study = optuna.create_study(
    direction="minimize", sampler=optuna.samplers.TPESampler(n_startup_trials=5, seed=0)
)
tpe_search_study.optimize(objective, n_trials=12, show_progress_bar=True)

print("Optimal set of hyperparameters:", tpe_search_study.best_params)
print(f"RMSE from optimal set of hyperparameters: {tpe_search_study.best_value:.5f}")
print("")
print(
    f"Hyperparameter tuning from TPE search reduced model error by {abs(1 - tpe_search_study.best_value/cnn_rmse):.1%}"
)

"""We can see that hyperparameter tuning reduced our model's error by a decent amount. Let's tally up the emissions cost from this exercise."""

tpe_search_emissions = tracker.stop()
print(f"Emissions from this training run: {tpe_search_emissions:.5f} kg CO2eq")

"""Finally, we'll take a quick look at the fit from a CNN trained with this optimal set of hyperparameters."""

def train_model_with_best_parameters(params):
    """
    Trains a CNN a given set of hyperparameters.

    Parameters:
      params (dict): Dictionary containing hyperparameter names and values

    Returns:
      (np.array, np.array): Tuple of predictions and test predictands
    """
    net = CNN(num_input_time_steps=params["num_input_time_steps"])

    train_predictors, train_predictands = assemble_predictors_predictands(
        start_date=train_start_date,
        end_date=train_end_date,
        lead_time=2,
        num_input_time_steps=params["num_input_time_steps"],
    )
    test_predictors, test_predictands = assemble_predictors_predictands(
        start_date="2007-01-01",
        end_date="2017-12-31",
        lead_time=2,
        num_input_time_steps=params["num_input_time_steps"],
    )

    train_dataset = ENSODataset(train_predictors, train_predictands)
    test_dataset = ENSODataset(test_predictors, test_predictands)

    trainloader = DataLoader(train_dataset, batch_size=10)
    testloader = DataLoader(test_dataset, batch_size=10)

    # Specify criterion and optimizer
    criterion = torch.nn.MSELoss()
    optimizer = getattr(optim, params["optimizer"])(
        net.parameters(), lr=params["learning_rate"]
    )

    cnn_train_session = TrainCNN(trainloader, testloader, net, criterion, optimizer)
    cnn_train_session.train_network(verbose=False)
    predictions = cnn_train_session.evaluate_best_model()
    return predictions, test_predictands

predictions, test_predictands = train_model_with_best_parameters(
    tpe_search_study.best_params
)
corr, _ = scipy.stats.pearsonr(predictions, test_predictands)

plot_nino_time_series(
    test_predictands,
    predictions,
    "Predicted and True Nino3.4 Indices on Training Set at 1 Month Lead Time. \n Corr: {:.2f}".format(
        corr
    ),
)

"""### **3.3.3 With Hyperparameter Pruning**

We might wish to stop training a CNN on a certain set of hyperparameters if the intermediate objective values don't look promising. For example, we might train our CNN on certain hyperparameters and notice that the RMSE by epoch 23 is quite poor. In that case, we should terminate this trial and move on to another set of hyperparameters. This is called "hyperparameter pruning".

To implement hyperparameter pruning, we'll need to adjust our CNN training code. Below I've added lines 36-41 to the function `train_network`. This new code calculates the RMSE at each epoch and calls the Optuna trial to determine whether to prune based on our specified pruning strategy (more on pruning strategies below).



"""

class TrainCNNOptuna(TrainCNN):
    def __init__(self, trainloader, testloader, net, criterion, optimizer):
        super().__init__(trainloader, testloader, net, criterion, optimizer)

    def train_network(self, trial, num_epochs=40, verbose=True):
        """
        Trains a neural network for a specified number of epochs with an
        ability to prune via Optuna.

        Parameters:
          trial (optuna.Trial): Provides interfaces to Optuna's functionality
          num_epochs (int): Number of epochs for which to train the network
          verbose (bool): Whether or not to print the current train and test loss

        Returns:
          None
        """
        net = self.net.to(self.device)

        for epoch in range(num_epochs):
            self._train()
            self._evaluate_epoch()
            if verbose:
                print(
                    "train Set: Epoch {:02d}. loss: {:3f}".format(
                        epoch + 1, self.train_losses[-1]
                    )
                )
                print(
                    "test Set: Epoch {:02d}. loss: {:3f}".format(
                        epoch + 1, self.test_losses[-1]
                    )
                )

            # Determine if trial should be pruned
            predictions = self.evaluate_best_model()
            test_predictands = self.testloader.dataset.predictands
            rmse = mean_squared_error(test_predictands, predictions, squared=False)
            trial.report(rmse, epoch)
            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()

        self.net = torch.load("current_best_cnn.pt")
        self.net.eval()
        self.net.to(self.device)

"""We also need to adjust our objective function to call this new training code."""

def objective_with_pruning(trial):
    """
    Defines the objective function to allow for pruning.

    Parameters:
      trial (optuna.Trial): Provides interfaces to Optuna's functionality

    Returns:
      (int): RMSE of current trial
    """
    params = {
        "num_input_time_steps": trial.suggest_int("num_input_time_steps", 1, 3),
        "learning_rate": trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True),
        "optimizer": trial.suggest_categorical("optimizer", ["Adam", "SGD"]),
    }

    net = CNN(num_input_time_steps=params["num_input_time_steps"])

    train_predictors, train_predictands = assemble_predictors_predictands(
        start_date=train_start_date,
        end_date=train_end_date,
        lead_time=1,
        num_input_time_steps=params["num_input_time_steps"],
    )
    test_predictors, test_predictands = assemble_predictors_predictands(
        start_date="2007-01-01",
        end_date="2017-12-31",
        lead_time=1,
        num_input_time_steps=params["num_input_time_steps"],
    )

    train_dataset = ENSODataset(train_predictors, train_predictands)
    test_dataset = ENSODataset(test_predictors, test_predictands)

    trainloader = DataLoader(train_dataset, batch_size=10)
    testloader = DataLoader(test_dataset, batch_size=10)

    # Specify criterion and optimizer
    criterion = torch.nn.MSELoss()
    optimizer = getattr(optim, params["optimizer"])(
        net.parameters(), lr=params["learning_rate"]
    )

    cnn_train_session = TrainCNNOptuna(
        trainloader, testloader, net, criterion, optimizer
    )
    cnn_train_session.train_network(trial, verbose=False)
    predictions = cnn_train_session.evaluate_best_model()
    rmse = mean_squared_error(test_predictands, predictions, squared=False)
    return rmse

"""Optuna offers several different hyperparameter pruning strategies.

- `ThresholdPruner`: Prunes a trial if the intermediate value (in our case, RMSE) is worse than a specified threshold at the same training epoch.
- `MedianPruner`: Prunes a trial if the intermediate value of the current trial is worse than the median of intermediate values from previous trials at the same training epoch.
- `PercentilePruner`: Prunes a trial if the intermediate value of the current trial is in the bottom percentile of trials at the same training epoch.
- `SuccessiveHalvingPruner`: Pruning strategy based on the [Successive Halving Algorithm (SHA)](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/). At a high level, the algorithm allocates more epochs to promising trials by by pruning half of the total trials.

Let's experiment with the `PercentilePruner`. We'll ask Optuna to prune when the RMSE falls below the bottom 10th percentile of previous trials.
"""

experiment_name = "tpe_search_percentile_pruned"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

tpe_search_10perc_pruned_study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=5, seed=0),
    pruner=optuna.pruners.PercentilePruner(percentile=10),
)
tpe_search_10perc_pruned_study.optimize(
    objective_with_pruning, n_trials=12, show_progress_bar=True
)

tpe_search_percentile_pruned_emissions = tracker.stop()
print(
    f"Emissions from this training run: {tpe_search_percentile_pruned_emissions:.5f} kg CO2eq"
)

"""Optuna ended up pruning half our total trials! Let's see if this strategy found a comparable hyperparameter set to our previous tuning study in which we never pruned."""

df = pd.DataFrame(
    {
        "strategy": ["Not pruned", "Pruned"],
        "num_input_time_steps": [
            tpe_search_study.best_params["num_input_time_steps"],
            tpe_search_10perc_pruned_study.best_params["num_input_time_steps"],
        ],
        "learning_rate": [
            tpe_search_study.best_params["learning_rate"],
            tpe_search_10perc_pruned_study.best_params["learning_rate"],
        ],
        "optimizer": [
            tpe_search_study.best_params["optimizer"],
            tpe_search_10perc_pruned_study.best_params["optimizer"],
        ],
        "RMSE": [
            tpe_search_study.best_value,
            tpe_search_10perc_pruned_study.best_value,
        ],
    }
)

df.set_index("strategy")

"""Our pruning strategy resulted in a slightly different set of optimal hyperparameters but with a similar RMSE!

### **3.3.4 Compare Emissions from Search Strategies**

Now let's compare emissions from both the pruned and the unpruned hyperparameter tuning studies.
"""

print(
    f"Pruning our search reduced emissions by {abs(1 - tpe_search_percentile_pruned_emissions / tpe_search_emissions):.1%}."
)

tuning_results = pd.DataFrame(
    {
        "Category": ["TPE Search", "TPE Search Pruned \n(10th percentile)"],
        "RMSE": [
            tpe_search_study.best_value,
            tpe_search_10perc_pruned_study.best_value,
        ],
        "Emissions": [
            tpe_search_emissions * 1000,
            tpe_search_percentile_pruned_emissions * 1000,
        ],
    }
)

plot_comparison(tuning_results)

"""The plot shows that our RMSE stayed essentially constant but we significantly reduced our emissions just by implementing a 10th percentile pruner. Not bad!

### Exercise 3
⭐ **YOUR TURN!:** Experiment with pruning using the `MedianPruner`. How does this change affect the number of trials pruned? What is the impact on the final emissions and model performance?
"""

experiment_name = "tpe_search_median_pruned"

tracker = EmissionsTracker(
    output_dir="./code_carbon/",
    output_file=f"{experiment_name}_emissions.csv",
    log_level="error",  # comment out this line to see regular output
)
tracker.start()

# Part 1: Use 5 start-up trials for both the TPE Sampler and the Median Pruner
tpe_search_median_pruned_study = optuna.create_study(
    ### YOUR CODE HERE ###
)
tpe_search_median_pruned_study.optimize(
    objective_with_pruning, n_trials=12, show_progress_bar=True
)

tpe_search_median_pruned_emissions = tracker.stop()
print(
    f"Emissions from this training run: {tpe_search_median_pruned_emissions:.5f} kg CO2eq"
)

df = pd.DataFrame(
    {
        "strategy": ["Not pruned", "Pruned: 10th Percentile", "Pruned: Median"],
        "num_input_time_steps": [
            tpe_search_study.best_params["num_input_time_steps"],
            tpe_search_10perc_pruned_study.best_params["num_input_time_steps"],
            tpe_search_median_pruned_study.best_params["num_input_time_steps"],

        ],
        "learning_rate": [
            tpe_search_study.best_params["learning_rate"],
            tpe_search_10perc_pruned_study.best_params["learning_rate"],
            tpe_search_median_pruned_study.best_params["learning_rate"],

        ],
        "optimizer": [
            tpe_search_study.best_params["optimizer"],
            tpe_search_10perc_pruned_study.best_params["optimizer"],
            tpe_search_median_pruned_study.best_params["optimizer"],

        ],
        "RMSE": [
            tpe_search_study.best_value,
            tpe_search_10perc_pruned_study.best_value,
            tpe_search_median_pruned_study.best_value,
        ],
        "Emissions": [
            tpe_search_emissions,
            tpe_search_percentile_pruned_emissions,
            tpe_search_median_pruned_emissions,
        ],
    }
)

df.set_index("strategy")

"""#### Solution"""

# Part 1
tpe_search_median_pruned_study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=5, seed=0),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5),
)

"""Fewer trials are pruned resulting in slightly higher emissions than the 10th percentile pruning strategy. The median pruning strategy found nearly the same optimal hyperparameters (and resulting RMSE) as when we didn't prune.

## **3.4 Other Exercises (Optional)**

Algorithm choice and hyperparameter tuning are just two strategies to reduce your emissions from training an ML model. If you'd like to continue to experiment, check out these optional challenge exercises!

* **Adjust your underlying hardware:** Click on "Runtime" at the top of this notebook and select "Change runtime type." Select "None" to switch to a runtime without any GPU's. Execute the entire notebook again and note how the emissions differ now that we're relying on CPU's to execute our ML training. Which type of models are most affected by this switch? If you have access to a Colab Pro account, try upgrading instead to a premium GPU class. How does access to faster GPU's affect the energy consumed by our code?
* **Adjust your compute region:** Click on "Runtime" at the top of this notebook and select "Disconnect and delete runtime." Execute the first two sections until you connect to a new compute region with a different energy intensity.

# **References**
[1] Kaack, L.H., Donti, P.L., Strubell, E. et al. "Aligning artificial intelligence with climate change mitigation." Nat. Clim. Chang. 12, 518–527 (2022). https://doi.org/10.1038/s41558-022-01377-7

[2] Oil in the Cloud: How Tech Companies are Helping Big Oil Profit from Climate Destruction (Greenpeace, 2019); https://www.greenpeace.org/usa/reports/oil-in-the-cloud/

[3] AI-Based Smart Farming: The Rise of Machine Learning in Livestock Farming (Nexocode, 2022); https://nexocode.com/blog/posts/ai-based-smart-farming-machine-learning-in-livestock-farming/

[4]  Dario Amodei and Daniel Hernandez. 2018. AI and Compute. https://openai.com/blog/ai-and-compute/

[5] Patterson, David A. et al. “The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.” Computer 55 (2022): 18-28.

[6] Budennyy, Semen A. et al. “eco2AI: Carbon Emissions Tracking of Machine Learning Models as the First Step Towards Sustainable AI.” Doklady Mathematics 106 (2022): S118 - S128.

[7] Wu, Carole-Jean et al. "Sustainable AI: Environmental Implications, Challenges and Opportunities." Proceedings of the 5th MLSys Conference, Santa Clara, CA, USA, 2022.

[8] Jiang, A. H. et al. Accelerating deep learning by focusing on the biggest losers. Preprint at https://arxiv.org/abs/1910.00762 (2019)

[9] ChatGPT reaches 100 million users two months after launch (The Guardian, 2023); https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app

[10] How much electricity does an American home use? (U.S. Energy Information Administration, 2022); https://www.eia.gov/tools/faqs/faq.php?id=97&t=3#

[11] Lannelongue, L., Grealey, J., Inouye, M. "Green Algorithms: Quantifying the carbon footprint of computation." Advanced Science 8(12), 2021. https://doi.org/10.1002/advs.202100707
"""